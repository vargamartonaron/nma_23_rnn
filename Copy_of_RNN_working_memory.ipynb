{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vargamartonaron/nma_23_rnn/blob/Excitation_Inhibition/Copy_of_RNN_working_memory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "1S71SBv7v2mR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "from scipy import stats\n",
        "\n",
        "import matplotlib as mpl\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "1IfN_hEpv2mT"
      },
      "outputs": [],
      "source": [
        "# Set some properties of the model. We'll store these in a dict so they're\n",
        "# easier to pass around or save.\n",
        "model = {}\n",
        "\n",
        "# properties of the recurrent pool:\n",
        "model['N'] = 1000  # number of neurons\n",
        "\n",
        "# model['g'] = 0.95  # gain of synaptic weights in pool\n",
        "model['g_ex'] = 0.95  # gain of synaptic weights in pool\n",
        "model['g_in'] = 0.95  # gain of synaptic weights in pool\n",
        "\n",
        "model['sp'] = 0.25  # fraction of weights that are nonzero\n",
        "model['tau'] = 20  # neural membrane time constant in ms\n",
        "model['dt'] = 0.1  # simulation timestep in ms\n",
        "model['nonlin'] = lambda x: np.tanh(x)  # firing rate nonlinearity for pool units\n",
        "\n",
        "# properties of the input layer:\n",
        "# a note: we're going to encode the \"value\" of the input by the identity of the\n",
        "# active input layer units. We'll use one-hot encoding: for each input step\n",
        "# during simulation, one unit will be activated with \"firing rate\" 1, and the\n",
        "# rest will be set to firing rate 0 (adjust gIn to change the scaling of input\n",
        "# to the recurrent pool.)\n",
        "# Note 1: This is just one way of setting up input- are there other approaches\n",
        "# that would improve memory capacity?\n",
        "# Note 2: Burn-in time is especially important if your model has g>1, in which\n",
        "# case neurons will be spontaneously active.\n",
        "model['nIn'] = 20  # size of the input layer\n",
        "model['gIn'] = 10.0  # gain of the input weights\n",
        "model['spIn'] = 0.05  # sparsity of input->pool connectivity\n",
        "model['burnIn'] = 10  # time before input starts\n",
        "model['durIn'] = 1  # time for which an input is active in ms\n",
        "model['ISI'] = 0  # time between inputs in ms\n",
        "model['nonlinIn'] = lambda x: x  # best to keep the input linear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "execution": {},
        "id": "aB8OlOxNv2mU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfc50222-49c2-4ebe-cd25-0b66d6f928f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.         -0.         -0.03173429 ... -0.          0.\n",
            "  -0.        ]\n",
            " [-0.00920693 -0.          0.         ... -0.          0.03889642\n",
            "   0.        ]\n",
            " [ 0.          0.01434764 -0.02692403 ... -0.         -0.\n",
            "  -0.        ]\n",
            " ...\n",
            " [ 0.          0.          0.         ... -0.05747478 -0.\n",
            "   0.        ]\n",
            " [ 0.          0.          0.         ... -0.09506025  0.\n",
            "  -0.        ]\n",
            " [-0.          0.06602794  0.         ... -0.          0.\n",
            "   0.        ]]\n"
          ]
        }
      ],
      "source": [
        "# Create the synaptic weight matrix.\n",
        "# Normalizing weights by sqrt(N*sparsity) keeps the eigenvalue spectrum\n",
        "# invariant to the size of the population N.\n",
        "randMat  = np.random.normal(0, 1, size=(model['N'], model['N']))\n",
        "spMat  = np.random.uniform(0, 1, size=(model['N'], model['N'])) <= model['sp']\n",
        "\n",
        "# Uniform gain\n",
        "# model['J'] = np.multiply(randMat, spMat) * model['g'] / math.sqrt(model['N'] * model['sp'])\n",
        "\n",
        "# Different excitatory and inhibitory gains\n",
        "model['J'] = np.multiply(randMat, spMat) / math.sqrt(model['N'] * model['sp'])\n",
        "model['J'] = np.where(model['J'] >= 0, model['J'] * model['g_ex'], model['J'] * model['g_in'])\n",
        "\n",
        "# Create the input weight matrix.\n",
        "randMatIn = np.random.normal(0, 1, size=(model['N'], model['nIn']))\n",
        "spMatIn = np.random.uniform(0, 1, size=(model['N'], model['nIn'])) <= model['spIn']\n",
        "model['Jin'] = np.multiply(randMatIn, spMatIn) * model['gIn'] / math.sqrt(model['nIn'] * model['spIn'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "Z8_wcsw6v2mV"
      },
      "outputs": [],
      "source": [
        "# Define a couple helper functions for simulation.\n",
        "\n",
        "def step(firing_rates, input_layer, model):\n",
        "  # The simulation function. We use Euler's method to simulate the evolution of\n",
        "  # model neuron firing rates given the input_layer firing rates.\n",
        "\n",
        "  timestep = math.exp(-model['dt']/model['tau'])\n",
        "  vIn = np.matmul(model['J'], firing_rates) \\\n",
        "        + np.matmul(model['Jin'], model['nonlinIn'](input_layer))\n",
        "  updated_rates = model['nonlin'](vIn + (firing_rates - vIn) * timestep)\n",
        "\n",
        "  return updated_rates\n",
        "\n",
        "\n",
        "def make_input(sequence_length, model):\n",
        "  # Generates a sequence of inputs according to the parameters in model. Returns\n",
        "  # the sequence both as a one-hot encoding and as a sequence of integer values.\n",
        "\n",
        "  input_stream = [0] * int(model['burnIn']/model['dt'])\n",
        "\n",
        "  for i in range(sequence_length):\n",
        "    val = np.random.randint(0, model['nIn']) + 1\n",
        "    for t in range(int(model['ISI']/model['dt'])):\n",
        "      input_stream.append(0.0)\n",
        "    for t in range(int(model['durIn']/model['dt'])):\n",
        "      input_stream.append(val)\n",
        "\n",
        "  input_stream = np.array(input_stream)\n",
        "\n",
        "  onehot = np.zeros((model['nIn'] + 1, input_stream.size))\n",
        "  onehot[input_stream, np.arange(input_stream.size)] = 1.0\n",
        "  onehot = onehot[1:, :]\n",
        "\n",
        "  return onehot, input_stream"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "n2BHqyemv2mV"
      },
      "outputs": [],
      "source": [
        "# Look at an example input stream.\n",
        "\n",
        "onehot, stream = make_input(50, model)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
        "omit = int(model['burnIn']/model['dt']) # don't plot the burn-in period\n",
        "ax[0].plot(np.arange(len(stream) - omit) * model['dt'], stream[omit:])\n",
        "ax[0].set_xlabel('time (ms)')\n",
        "ax[0].set_ylabel('input value')\n",
        "\n",
        "ax[1].imshow(onehot[:, omit:], aspect='auto')\n",
        "ax[1].set_xlabel('time (ms)')\n",
        "ax[1].set_ylabel('input one-hot encoding')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "1Tyh7zycv2mW"
      },
      "outputs": [],
      "source": [
        "# Take a look at the eigenvalue spectrum of J.\n",
        "w, v = np.linalg.eig(model['J'])\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
        "showCount = 50  # portion of J to actually show (for readability)\n",
        "h = ax[0].imshow(model['J'][:showCount,:showCount])\n",
        "ax[0].set_title('Sample from weight matrix J')\n",
        "ax[0].set_xlabel('presynaptic neuron')\n",
        "ax[0].set_ylabel('postsynaptic neuron')\n",
        "plt.colorbar(h, ax=ax[0])\n",
        "\n",
        "ax[1].plot(np.real(w),np.imag(w),'.')\n",
        "ax[1].plot(np.sin(np.linspace(0,2*math.pi,100)),\n",
        "           np.cos(np.linspace(0,2*math.pi,100)))  # circle with radius 1\n",
        "ax[1].set_title('Eigenvalue spectrum of J')\n",
        "ax[1].set_xlabel('real component')\n",
        "ax[1].set_ylabel('imaginary component')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "GUEw1aj9v2mW"
      },
      "outputs": [],
      "source": [
        "# Simulate the model activity.\n",
        "\n",
        "# generate the input to the model\n",
        "n_stimuli = 1000\n",
        "onehot, input_stream = make_input(n_stimuli, model)\n",
        "\n",
        "# initialize the firing rates randomly\n",
        "firing_rates = np.zeros((model['N'], len(input_stream)))\n",
        "firing_rates[:, 0] = np.random.uniform(0, 0.1, size=(model['N']))\n",
        "\n",
        "for t in range(len(input_stream)-1):\n",
        "  firing_rates[:,t+1] = step(firing_rates[:,t], onehot[:,t], model)\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(2, 1, figsize=(8, 12))\n",
        "simulation_time = np.arange(len(input_stream))*model['dt'] - model['burnIn']\n",
        "ax[0].plot(simulation_time, input_stream)\n",
        "ax[0].set_xlabel('Time (ms)')\n",
        "ax[0].set_ylabel('Input value')\n",
        "\n",
        "extents = [simulation_time[0],simulation_time[-1], 0, model['N']]\n",
        "ax[1].imshow(firing_rates, aspect='auto', extent=extents)\n",
        "ax[1].set_xlabel('Time (ms)')\n",
        "ax[1].set_ylabel('Neurons')\n",
        "fig.show()\n",
        "\n",
        "onehot"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make data to analyze\n",
        "burnIn_steps = int(model['burnIn']/model['dt'])\n",
        "durIn_steps =int(model['durIn']/model['dt'])\n",
        "\n",
        "stimuli = np.arange(1, model['nIn']+1)\n",
        "resp_0 = np.zeros((model['nIn']+1, model['N']))\n",
        "\n",
        "# For each neuron avarage over its activity at the last step of durIn\n",
        "# of inputs of each of 20 inputs\n",
        "for i in range(1, model['nIn']+1):\n",
        "  # Take only the index of the last timestep of each input period\n",
        "  this_input_idx = np.where(input_stream == i)[0][durIn_steps-1::durIn_steps]\n",
        "  resp_0[i] = np.mean(firing_rates[:,this_input_idx], 1)\n",
        "\n",
        "# For each neuron z-score over all 20 inputs\n",
        "resp_0 = stats.zscore(resp_0[1:], axis=0, nan_policy='omit')"
      ],
      "metadata": {
        "id": "Hxe8CiER0bUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resp_tensor = torch.tensor(resp_0, dtype=torch.float32)\n",
        "stimuli_tensor = torch.tensor(stimuli, dtype=torch.float32).unsqueeze(1)  # add singleton dimension to make a column vector"
      ],
      "metadata": {
        "id": "rp5SXxpkxiC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seeds for reproducibility\n",
        "np.random.seed(4)\n",
        "torch.manual_seed(4)\n",
        "\n",
        "n_stimuli = len(stimuli)\n",
        "\n",
        "# Split data into training set and testing set\n",
        "n_train = int(0.6 * n_stimuli)  # use 60% of all data for training set\n",
        "ishuffle = torch.randperm(n_stimuli)\n",
        "itrain = ishuffle[:n_train]  # indices of data samples to include in training set\n",
        "itest = ishuffle[n_train:]  # indices of data samples to include in testing set\n",
        "stimuli_test = stimuli_tensor[itest]\n",
        "resp_test = resp_tensor[itest]\n",
        "stimuli_train = stimuli_tensor[itrain]\n",
        "resp_train = resp_tensor[itrain]"
      ],
      "metadata": {
        "id": "q8s1LxCgyMa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepNetReLU(nn.Module):\n",
        "  \"\"\" network with a single hidden layer h with a RELU \"\"\"\n",
        "\n",
        "  def __init__(self, n_inputs, n_hidden):\n",
        "    super().__init__()  # needed to invoke the properties of the parent class nn.Module\n",
        "    self.in_layer = nn.Linear(n_inputs, n_hidden) # neural activity --> hidden units\n",
        "    self.out_layer = nn.Linear(n_hidden, 1) # hidden units --> output\n",
        "\n",
        "  def forward(self, r):\n",
        "\n",
        "    h = torch.relu(self.in_layer(r)) # h is size (n_inputs, n_hidden)\n",
        "    y = self.out_layer(h) # y is size (n_inputs, 1)\n",
        "\n",
        "    return y"
      ],
      "metadata": {
        "id": "oIuKS0pcxC_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_train_loss(train_loss, test_loss):\n",
        "  plt.plot(train_loss)\n",
        "  plt.plot(test_loss)\n",
        "  plt.xlim([0, None])\n",
        "  plt.ylim([0, None])\n",
        "  plt.xlabel('iterations of gradient descent')\n",
        "  plt.ylabel('mean squared error')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "I6_aLk_1Ndxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net, loss_fn, train_data, train_labels, test_data, test_labels,\n",
        "          n_epochs=500, learning_rate=1e-4):\n",
        "\n",
        "  # Initialize PyTorch SGD optimizer\n",
        "  optimizer = optim.SGD(net.parameters(), lr=learning_rate)\n",
        "\n",
        "  # Placeholder to save the loss at each iteration\n",
        "  train_loss = []\n",
        "  test_loss = []\n",
        "\n",
        "  # Loop over epochs\n",
        "  for i in range(n_epochs):\n",
        "\n",
        "    # compute network output from inputs in train_data\n",
        "    out = net(train_data)  # compute network output from inputs in train_data\n",
        "\n",
        "    # evaluate loss function\n",
        "    loss = loss_fn(out, train_labels)\n",
        "\n",
        "    # Clear previous gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Compute gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # Update weights\n",
        "    optimizer.step()\n",
        "\n",
        "    # Store current value of loss\n",
        "    train_loss.append(loss.item())  # .item() needed to transform the tensor output of loss_fn to a scalar\n",
        "\n",
        "    # compute network output from inputs in test_data\n",
        "    out = net(test_data)  # compute network output from inputs in test_data\n",
        "\n",
        "    # evaluate loss function\n",
        "    loss = loss_fn(out, test_labels)\n",
        "\n",
        "    test_loss.append(loss.item())\n",
        "\n",
        "    # Track progress\n",
        "    if (i + 1) % (n_epochs // 5) == 0:\n",
        "      print(f'iteration {i + 1}/{n_epochs} | loss: {loss.item():.3f}')\n",
        "\n",
        "  return train_loss, test_loss\n",
        "\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# Initialize network with 10 hidden units\n",
        "net = DeepNetReLU(1000, 10)\n",
        "\n",
        "# Initialize built-in PyTorch MSE loss function\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# Run gradient descent on data\n",
        "train_loss, test_loss = train(net, loss_fn, resp_train, stimuli_train, resp_test, stimuli_test)\n",
        "\n",
        "# Plot the training loss over iterations of GD\n",
        "plot_train_loss(train_loss, test_loss)"
      ],
      "metadata": {
        "id": "OmO6-Ej8w1Nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "e5LW3QGCv2mX"
      },
      "source": [
        "Now: can you decode the model's input history from its firing rates?"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "00c-ENUN5KBS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}