{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vargamartonaron/nma_23_rnn/blob/main/working_memory_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the OU process function for generating input data\n",
        "def ou_process_function(x, mu, theta, sigma):\n",
        "    dx = theta * (mu - x) + sigma * torch.randn_like(x)\n",
        "    return x + dx\n",
        "\n",
        "# Define the InputLayer, RNNLayer, and OutputLayer classes\n",
        "class InputLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(InputLayer, self).__init__()\n",
        "        self.size = 20\n",
        "        self.gain = 10.0\n",
        "        self.time_before_input_starts = 10\n",
        "        self.time_for_input_active = 1\n",
        "        self.fc = nn.Linear(self.size, self.size, bias=False)\n",
        "        self.input_data = torch.zeros(self.size)\n",
        "        self.time_step_counter = 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Increment the time step counter\n",
        "        self.time_step_counter += 1\n",
        "\n",
        "        if self.time_step_counter >= self.time_before_input_starts and self.time_step_counter < self.time_before_input_starts + self.time_for_input_active:\n",
        "            self.input_data = ou_process_function(self.input_data, mu=1, theta=0.05, sigma=0.1)\n",
        "        else:\n",
        "            self.input_data = torch.zeros(self.size)\n",
        "\n",
        "        # Reset the time step counter after 10 milliseconds\n",
        "        if self.time_step_counter == self.time_before_input_starts + self.time_for_input_active:\n",
        "            self.time_step_counter = 0\n",
        "\n",
        "        return self.gain * self.fc(self.input_data)\n",
        "\n",
        "class RNNLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RNNLayer, self).__init__()\n",
        "        self.num_neurons = 2000\n",
        "        self.gain_synaptic_weights = 0.95\n",
        "        self.fraction_nonzero_weights = 0.25\n",
        "        self.tau = 20\n",
        "        self.dt = 0.1\n",
        "        self.sparsity = 0.05\n",
        "        self.fc = nn.Linear(self.num_neurons, self.num_neurons, bias=False)\n",
        "        self.nonlinearity = nn.Tanh()\n",
        "        self.weight_init()\n",
        "\n",
        "    def weight_init(self):\n",
        "        # Initialize the dense weight tensor with zeros\n",
        "        self.fc.weight.data = torch.zeros(self.num_neurons, self.num_neurons)\n",
        "\n",
        "        # Set a fraction of the weights to non-zero values to achieve sparsity\n",
        "        num_nonzero_weights = int(self.num_neurons * self.num_neurons * self.sparsity)\n",
        "        indices = torch.randint(0, self.num_neurons, size=(2, num_nonzero_weights))\n",
        "        values = torch.randn(num_nonzero_weights) * self.gain_synaptic_weights\n",
        "        self.fc.weight.data[indices[0], indices[1]] = values\n",
        "\n",
        "    def forward(self, x, state):\n",
        "        dx = (-state + self.fc(self.nonlinearity(state))) / self.tau\n",
        "        state = torch.add(state, self.dt * dx)\n",
        "        return state\n",
        "\n",
        "class OutputLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(OutputLayer, self).__init__()\n",
        "        self.fc = nn.Linear(2000, 20, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Define the complete WorkingMemoryModel class\n",
        "class WorkingMemoryModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(WorkingMemoryModel, self).__init__()\n",
        "        self.input_layer = InputLayer()\n",
        "        self.rnn_layer = RNNLayer()\n",
        "        self.output_layer = OutputLayer()\n",
        "\n",
        "    def forward(self, x):\n",
        "        rnn_state = torch.zeros(self.rnn_layer.num_neurons)\n",
        "        inputs = torch.stack([self.input_layer(t) for t in x])\n",
        "        for input_t in inputs:\n",
        "            rnn_state = self.rnn_layer(input_t, rnn_state)\n",
        "        output = self.output_layer(rnn_state)\n",
        "        return output"
      ],
      "metadata": {
        "id": "BfBgZabfRTjQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e386603-f0c2-45d2-ad98-8386fc119130"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_model_with_rnn_state(model, total_time):\n",
        "    input_data = torch.stack([model.input_layer(t) for t in total_time])\n",
        "    rnn_state = torch.zeros(model.rnn_layer.num_neurons)\n",
        "\n",
        "    outputs = []\n",
        "    for input_t in input_data:\n",
        "        rnn_state = model.rnn_layer(input_t, rnn_state)\n",
        "        output = model.output_layer(rnn_state)\n",
        "        outputs.append(output)\n",
        "\n",
        "    return torch.stack(outputs), rnn_state"
      ],
      "metadata": {
        "id": "imkYC2R0Td0V"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, total_time, num_epochs):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Enable anomaly detection\n",
        "    autograd.set_detect_anomaly(True)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        optimizer.zero_grad()\n",
        "        output, _ = simulate_model_with_rnn_state(model, total_time)\n",
        "\n",
        "        # Generate the input data for the current time step\n",
        "        input_data = torch.stack([model.input_layer(t) for t in total_time])\n",
        "\n",
        "        # Calculate the loss between the generated input and the decoded output\n",
        "        loss = criterion(input_data, output)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "# Example usage for training the model\n",
        "model = WorkingMemoryModel()\n",
        "total_time = 10.0\n",
        "total_time = [t * 0.1 for t in range(int(total_time * 10))]  # Adjust the total time based on your requirement\n",
        "num_epochs = 200\n",
        "\n",
        "train_model(model, total_time, num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaSGc-xaWF9U",
        "outputId": "3d95b72e-679c-421b-ea99-b05d268c2d6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200, Loss: 0.035526808351278305\n",
            "Epoch 2/200, Loss: 0.042763762176036835\n",
            "Epoch 3/200, Loss: 0.030870338901877403\n",
            "Epoch 4/200, Loss: 0.04346989095211029\n",
            "Epoch 5/200, Loss: 0.03793623298406601\n",
            "Epoch 6/200, Loss: 0.036768969148397446\n",
            "Epoch 7/200, Loss: 0.03384675085544586\n",
            "Epoch 8/200, Loss: 0.030449654906988144\n",
            "Epoch 9/200, Loss: 0.04049912095069885\n",
            "Epoch 10/200, Loss: 0.03141278028488159\n",
            "Epoch 11/200, Loss: 0.025123581290245056\n",
            "Epoch 12/200, Loss: 0.030217895284295082\n",
            "Epoch 13/200, Loss: 0.029994338750839233\n",
            "Epoch 14/200, Loss: 0.027675170451402664\n",
            "Epoch 15/200, Loss: 0.03106709197163582\n",
            "Epoch 16/200, Loss: 0.03222484514117241\n",
            "Epoch 17/200, Loss: 0.02783745899796486\n",
            "Epoch 18/200, Loss: 0.03632538020610809\n",
            "Epoch 19/200, Loss: 0.027975425124168396\n",
            "Epoch 20/200, Loss: 0.02659611403942108\n",
            "Epoch 21/200, Loss: 0.023287536576390266\n",
            "Epoch 22/200, Loss: 0.027468902990221977\n",
            "Epoch 23/200, Loss: 0.026698697358369827\n",
            "Epoch 24/200, Loss: 0.022970542311668396\n",
            "Epoch 25/200, Loss: 0.021488534286618233\n",
            "Epoch 26/200, Loss: 0.026309536769986153\n",
            "Epoch 27/200, Loss: 0.023175708949565887\n",
            "Epoch 28/200, Loss: 0.03159444034099579\n",
            "Epoch 29/200, Loss: 0.02939879707992077\n",
            "Epoch 30/200, Loss: 0.02939249761402607\n",
            "Epoch 31/200, Loss: 0.022882871329784393\n",
            "Epoch 32/200, Loss: 0.02505827508866787\n"
          ]
        }
      ]
    }
  ]
}